{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1337 D00M\n",
    "\n",
    "### description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.enable_eager_execution()\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our a MDP environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"take_cover.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"take_cover.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    game.set_ticrate(200)\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0]\n",
    "    right = [0, 1]\n",
    "    possible_actions = [left, right]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode temporal relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    #print(frame.shape)\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 1000              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an experience replay mechanism to make more efficient use of observed experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # Get the first frame\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)  # Random action\n",
    "    reward = game.make_action(action)         # Get the rewards\n",
    "    done = game.is_episode_finished()         # Look if the episode is finished\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)                     # The episode is over\n",
    "        memory.add((state, action, reward, next_state, done))  # Add experience to memory\n",
    "        game.new_episode()                                     # Start a new episode\n",
    "        state = game.get_state().screen_buffer                 # Get the first frame\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True) # Stack the frames\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer            # Get the next state\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        memory.add((state, action, reward, next_state, done))  # Add experience to memory\n",
    "        state = next_state                                     # Update current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a block of CNN's for Q-value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnEluBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel, strides, padding):\n",
    "        super(ConvBnEluBlock, self).__init__()\n",
    "        self.cnn = tf.keras.layers.Conv2D(filters, (kernel, kernel), strides=(strides, strides), padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        self.bn = tf.keras.layers.BatchNormalization(trainable=True, epsilon=1e-5)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.cnn(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = tf.nn.elu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.block1 = ConvBnEluBlock(32, kernel=8, strides=4, padding='valid') \n",
    "        self.block2 = ConvBnEluBlock(64, kernel=4, strides=2, padding='valid')\n",
    "        self.block3 = ConvBnEluBlock(128, kernel=4, strides=2, padding='valid')\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(units=512, activation=tf.nn.elu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.out = tf.keras.layers.Dense(units=num_actions, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, plot=False):\n",
    "        x = self.block1(inputs) # [84, 84, 4] -> [20, 20, 32]\n",
    "        if plot:\n",
    "            return x\n",
    "        x = self.block2(x) # [20, 20, 32] ->  [9, 9, 64]\n",
    "        x = self.block3(x) #  [9, 9, 64] -> [3, 3, 128]\n",
    "        x = self.flatten(x) # [9, 9, 64] -> [1152]\n",
    "        x = self.fc(x) # [1152] -> [512]\n",
    "        output = self.out(x) # [512] -> [2]\n",
    "\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $ϵ$ select a random action $a_t$, otherwise select $a_t=argmax_{a}Q(s',a')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions, model):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = model(tf.cast(np.expand_dims(state, 0), tf.float32))\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "logdir = \"/home/arialinvlad/Deep_reinforcement_learning_Course/DQL/Doom/take_cover/tb/\"\n",
    "writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "writer.set_as_default()\n",
    "\n",
    "# for i in range(10):\n",
    "#     # Must include a record_summaries method\n",
    "#     with tf.contrib.summary.record_summaries_every_n_global_steps(3):\n",
    "#         global_step.assign_add(1)\n",
    "#         # your model code goes here\n",
    "#         print(global_step)\n",
    "#         opertaion = tf.contrib.summary.scalar('loss', i)\n",
    "#         print(opertaion)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNetwork(num_actions=2)\n",
    "saver = 0\n",
    "\n",
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 107.0 Training loss: 1.0791 Explore P: 0.9895\n",
      "Episode: 1 Total reward: 50.0 Training loss: 2.6236 Explore P: 0.9846\n",
      "Episode: 2 Total reward: 83.0 Training loss: 0.6040 Explore P: 0.9765\n",
      "Episode: 3 Total reward: 94.0 Training loss: 0.5058 Explore P: 0.9675\n",
      "Episode: 4 Total reward: 48.0 Training loss: 0.5009 Explore P: 0.9629\n",
      "Episode: 5 Total reward: 73.0 Training loss: 0.5474 Explore P: 0.9560\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 48.0 Training loss: 0.3184 Explore P: 0.9514\n",
      "Episode: 7 Total reward: 53.0 Training loss: 0.5707 Explore P: 0.9465\n",
      "Episode: 8 Total reward: 53.0 Training loss: 0.0145 Explore P: 0.9415\n",
      "Episode: 9 Total reward: 83.0 Training loss: 0.1880 Explore P: 0.9338\n",
      "Episode: 10 Total reward: 49.0 Training loss: 0.0495 Explore P: 0.9293\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 47.0 Training loss: 0.0424 Explore P: 0.9250\n",
      "Episode: 12 Total reward: 114.0 Training loss: 0.0158 Explore P: 0.9146\n",
      "Episode: 13 Total reward: 137.0 Training loss: 0.1796 Explore P: 0.9023\n",
      "Episode: 14 Total reward: 93.0 Training loss: 0.1209 Explore P: 0.8940\n",
      "Episode: 15 Total reward: 96.0 Training loss: 0.0264 Explore P: 0.8856\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 52.0 Training loss: 0.0079 Explore P: 0.8811\n",
      "Episode: 17 Total reward: 48.0 Training loss: 0.1033 Explore P: 0.8769\n",
      "Episode: 18 Total reward: 47.0 Training loss: 0.0979 Explore P: 0.8728\n",
      "Episode: 19 Total reward: 90.0 Training loss: 0.0098 Explore P: 0.8651\n",
      "Episode: 20 Total reward: 88.0 Training loss: 0.0439 Explore P: 0.8576\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 83.0 Training loss: 0.0893 Explore P: 0.8506\n",
      "Episode: 22 Total reward: 48.0 Training loss: 0.0251 Explore P: 0.8466\n",
      "Episode: 23 Total reward: 67.0 Training loss: 0.0689 Explore P: 0.8410\n",
      "Episode: 24 Total reward: 52.0 Training loss: 0.0208 Explore P: 0.8367\n",
      "Episode: 25 Total reward: 85.0 Training loss: 0.0113 Explore P: 0.8297\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 88.0 Training loss: 0.0161 Explore P: 0.8225\n",
      "Episode: 27 Total reward: 59.0 Training loss: 0.0012 Explore P: 0.8177\n",
      "Episode: 28 Total reward: 53.0 Training loss: 0.0573 Explore P: 0.8134\n",
      "Episode: 29 Total reward: 62.0 Training loss: 0.0072 Explore P: 0.8085\n",
      "Episode: 30 Total reward: 50.0 Training loss: 0.0229 Explore P: 0.8045\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 47.0 Training loss: 0.2177 Explore P: 0.8008\n",
      "Episode: 32 Total reward: 52.0 Training loss: 0.0114 Explore P: 0.7967\n",
      "Episode: 33 Total reward: 50.0 Training loss: 0.0074 Explore P: 0.7927\n",
      "Episode: 34 Total reward: 55.0 Training loss: 0.0172 Explore P: 0.7885\n",
      "Episode: 35 Total reward: 88.0 Training loss: 0.0211 Explore P: 0.7816\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 57.0 Training loss: 0.0207 Explore P: 0.7772\n",
      "Episode: 37 Total reward: 129.0 Training loss: 0.0043 Explore P: 0.7674\n",
      "Episode: 38 Total reward: 106.0 Training loss: 0.0184 Explore P: 0.7594\n",
      "Episode: 39 Total reward: 55.0 Training loss: 0.0201 Explore P: 0.7553\n",
      "Episode: 40 Total reward: 56.0 Training loss: 0.0245 Explore P: 0.7512\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 96.0 Training loss: 0.0196 Explore P: 0.7441\n",
      "Episode: 42 Total reward: 118.0 Training loss: 0.0272 Explore P: 0.7355\n",
      "Episode: 43 Total reward: 51.0 Training loss: 0.0046 Explore P: 0.7318\n",
      "Episode: 44 Total reward: 125.0 Training loss: 0.0184 Explore P: 0.7228\n",
      "Episode: 45 Total reward: 60.0 Training loss: 0.0127 Explore P: 0.7185\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 54.0 Training loss: 0.0223 Explore P: 0.7147\n",
      "Episode: 47 Total reward: 143.0 Training loss: 0.0072 Explore P: 0.7047\n",
      "Episode: 48 Total reward: 48.0 Training loss: 0.0027 Explore P: 0.7014\n",
      "Episode: 49 Total reward: 71.0 Training loss: 0.0069 Explore P: 0.6965\n",
      "Episode: 50 Total reward: 132.0 Training loss: 0.0545 Explore P: 0.6875\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 48.0 Training loss: 0.0114 Explore P: 0.6843\n",
      "Episode: 52 Total reward: 77.0 Training loss: 0.0143 Explore P: 0.6791\n",
      "Episode: 53 Total reward: 65.0 Training loss: 0.0046 Explore P: 0.6747\n",
      "Episode: 54 Total reward: 123.0 Training loss: 0.0124 Explore P: 0.6666\n",
      "Episode: 55 Total reward: 56.0 Training loss: 0.0119 Explore P: 0.6630\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 74.0 Training loss: 0.0070 Explore P: 0.6581\n",
      "Episode: 57 Total reward: 70.0 Training loss: 0.0261 Explore P: 0.6536\n",
      "Episode: 58 Total reward: 140.0 Training loss: 0.0041 Explore P: 0.6447\n",
      "Episode: 59 Total reward: 48.0 Training loss: 0.0119 Explore P: 0.6416\n",
      "Episode: 60 Total reward: 94.0 Training loss: 0.0208 Explore P: 0.6357\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 63.0 Training loss: 0.0280 Explore P: 0.6318\n",
      "Episode: 62 Total reward: 90.0 Training loss: 0.0133 Explore P: 0.6262\n",
      "Episode: 63 Total reward: 143.0 Training loss: 0.0049 Explore P: 0.6175\n",
      "Episode: 64 Total reward: 53.0 Training loss: 0.0047 Explore P: 0.6143\n",
      "Episode: 65 Total reward: 71.0 Training loss: 0.0182 Explore P: 0.6100\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 50.0 Training loss: 0.0083 Explore P: 0.6070\n",
      "Episode: 67 Total reward: 47.0 Training loss: 0.0224 Explore P: 0.6042\n",
      "Episode: 68 Total reward: 70.0 Training loss: 0.0047 Explore P: 0.6000\n",
      "Episode: 69 Total reward: 71.0 Training loss: 0.0060 Explore P: 0.5959\n",
      "Episode: 70 Total reward: 56.0 Training loss: 0.0611 Explore P: 0.5926\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 89.0 Training loss: 0.0080 Explore P: 0.5874\n",
      "Episode: 72 Total reward: 53.0 Training loss: 0.0021 Explore P: 0.5844\n",
      "Episode: 73 Total reward: 88.0 Training loss: 0.0072 Explore P: 0.5794\n",
      "Episode: 74 Total reward: 54.0 Training loss: 0.0056 Explore P: 0.5763\n",
      "Episode: 75 Total reward: 63.0 Training loss: 0.0118 Explore P: 0.5727\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 68.0 Training loss: 0.0011 Explore P: 0.5689\n",
      "Episode: 77 Total reward: 63.0 Training loss: 0.0040 Explore P: 0.5654\n",
      "Episode: 78 Total reward: 101.0 Training loss: 0.0096 Explore P: 0.5598\n",
      "Episode: 79 Total reward: 52.0 Training loss: 0.0069 Explore P: 0.5570\n",
      "Episode: 80 Total reward: 91.0 Training loss: 0.0036 Explore P: 0.5520\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 101.0 Training loss: 0.0107 Explore P: 0.5466\n",
      "Episode: 82 Total reward: 66.0 Training loss: 0.0097 Explore P: 0.5430\n",
      "Episode: 83 Total reward: 63.0 Training loss: 0.0066 Explore P: 0.5397\n",
      "Episode: 84 Total reward: 55.0 Training loss: 0.0035 Explore P: 0.5368\n",
      "Episode: 85 Total reward: 62.0 Training loss: 0.0070 Explore P: 0.5335\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 53.0 Training loss: 0.0102 Explore P: 0.5308\n",
      "Episode: 87 Total reward: 53.0 Training loss: 0.0074 Explore P: 0.5280\n",
      "Episode: 88 Total reward: 50.0 Training loss: 0.0050 Explore P: 0.5254\n",
      "Episode: 89 Total reward: 54.0 Training loss: 0.0062 Explore P: 0.5227\n",
      "Episode: 90 Total reward: 87.0 Training loss: 0.0045 Explore P: 0.5182\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 65.0 Training loss: 0.0025 Explore P: 0.5149\n",
      "Episode: 92 Total reward: 63.0 Training loss: 0.0027 Explore P: 0.5118\n",
      "Episode: 93 Total reward: 56.0 Training loss: 0.0037 Explore P: 0.5089\n",
      "Episode: 94 Total reward: 82.0 Training loss: 0.0038 Explore P: 0.5049\n",
      "Episode: 95 Total reward: 96.0 Training loss: 0.0068 Explore P: 0.5001\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 99.0 Training loss: 0.0028 Explore P: 0.4953\n",
      "Episode: 97 Total reward: 77.0 Training loss: 0.0112 Explore P: 0.4916\n",
      "Episode: 98 Total reward: 105.0 Training loss: 0.0043 Explore P: 0.4866\n",
      "Episode: 99 Total reward: 51.0 Training loss: 0.0009 Explore P: 0.4841\n",
      "Episode: 100 Total reward: 52.0 Training loss: 0.0049 Explore P: 0.4817\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 77.0 Training loss: 0.0044 Explore P: 0.4781\n",
      "Episode: 102 Total reward: 88.0 Training loss: 0.0047 Explore P: 0.4740\n",
      "Episode: 103 Total reward: 86.0 Training loss: 0.0803 Explore P: 0.4700\n",
      "Episode: 104 Total reward: 57.0 Training loss: 0.0037 Explore P: 0.4674\n",
      "Episode: 105 Total reward: 69.0 Training loss: 0.0040 Explore P: 0.4642\n",
      "Model Saved\n",
      "Episode: 106 Total reward: 49.0 Training loss: 0.0069 Explore P: 0.4620\n",
      "Episode: 107 Total reward: 47.0 Training loss: 0.0012 Explore P: 0.4599\n",
      "Episode: 108 Total reward: 55.0 Training loss: 0.0106 Explore P: 0.4574\n",
      "Episode: 109 Total reward: 89.0 Training loss: 0.0039 Explore P: 0.4535\n",
      "Episode: 110 Total reward: 161.0 Training loss: 0.0039 Explore P: 0.4464\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 50.0 Training loss: 0.0040 Explore P: 0.4442\n",
      "Episode: 112 Total reward: 50.0 Training loss: 0.0012 Explore P: 0.4420\n",
      "Episode: 113 Total reward: 54.0 Training loss: 0.0041 Explore P: 0.4397\n",
      "Episode: 114 Total reward: 65.0 Training loss: 0.0048 Explore P: 0.4369\n",
      "Episode: 115 Total reward: 51.0 Training loss: 0.0088 Explore P: 0.4348\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 51.0 Training loss: 0.0048 Explore P: 0.4326\n",
      "Episode: 117 Total reward: 51.0 Training loss: 0.0030 Explore P: 0.4304\n",
      "Episode: 118 Total reward: 55.0 Training loss: 0.0036 Explore P: 0.4281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-ccb19d98dd12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mQs_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   1671\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   1672\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         _ctx._post_execution_callbacks, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[0m\u001b[1;32m   1674\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "with tf.contrib.summary.always_record_summaries():\n",
    "\n",
    "    if training == True:\n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "\n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "\n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "\n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "\n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "\n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, model)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "\n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "\n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    Qs_next_state = model(tf.cast(next_states_mb, tf.float32), training=True)\n",
    "                    if not saver:\n",
    "                        saver = tfe.Saver(model.variables)\n",
    "\n",
    "\n",
    "                    # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                    for i in range(0, len(batch)):\n",
    "                        terminal = dones_mb[i]\n",
    "\n",
    "                        # If we are in a terminal state, only equals reward\n",
    "                        if terminal:\n",
    "                            target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                        else:\n",
    "                            target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                            target_Qs_batch.append(target)\n",
    "\n",
    "\n",
    "                    targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                    Q = tf.reduce_sum(tf.multiply(Qs_next_state, actions_mb), axis=1)\n",
    "                    loss = tf.reduce_mean(tf.square(targets_mb - Q))\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#                 operation = tf.contrib.summary.scalar('loss', loss)\n",
    "#                 print(operation)\n",
    "\n",
    "            if episode % 5 == 0 and episode:\n",
    "                saver.save(\"./models/model.ckpt\", global_step=global_step)\n",
    "                print(\"Model Saved\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'restore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-03094fc0ef08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/model.ckpt-0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'restore'"
     ]
    }
   ],
   "source": [
    "game, possible_actions = create_environment()\n",
    "\n",
    "totalScore = 0\n",
    "\n",
    "# Load the model\n",
    "saver.restore(\"./models/model.ckpt-0\")\n",
    "game.init()\n",
    "for i in range(1):\n",
    "\n",
    "    done = False\n",
    "\n",
    "    game.new_episode()\n",
    "\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        Qs = model(tf.cast(np.expand_dims(state, 0), tf.float32))\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "\n",
    "        game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "        score = game.get_total_reward()\n",
    "\n",
    "        if done:\n",
    "            break  \n",
    "\n",
    "        else:\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "    score = game.get_total_reward()\n",
    "    print(\"Score: \", score)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_weights(weights, input_channel=0):\n",
    "    # Assume weights are TensorFlow ops for 4-dim variables\n",
    "    # e.g. weights_conv1 or weights_conv2.\n",
    "\n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.array(tf.reduce_min(weights))\n",
    "    w_max = np.array(tf.reduce_max(weights))\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = weights.shape[3].value\n",
    "\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid filter-weights.\n",
    "        if i<num_filters:\n",
    "            # Get the weights for the i'th filter of the input channel.\n",
    "            # See new_conv_layer() for details on the format\n",
    "            # of this 4-dim tensor.\n",
    "            img = weights[:, :, input_channel, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
    "                      interpolation='nearest', cmap='seismic')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conv_weights(model.block1.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image.reshape((84, 84)),\n",
    "               interpolation='nearest',\n",
    "               cmap='binary')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_layer(layer, image):\n",
    "    # Assume layer is a TensorFlow op that outputs a 4-dim tensor\n",
    "    # which is the output of a convolutional layer,\n",
    "    # e.g. layer_conv1 or layer_conv2.\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = layer.shape[3].value\n",
    "\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot the output images of all the filters.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the images for valid filters.\n",
    "        if i<num_filters:\n",
    "            # Get the output image of using the i'th filter.\n",
    "            # See new_conv_layer() for details on the format\n",
    "            # of this 4-dim tensor.\n",
    "            img = layer[0, :, :, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, interpolation='nearest', cmap='binary')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = memory.sample(batch_size)\n",
    "next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "inputs = model(tf.cast(next_states_mb, tf.float32), training=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 140.0\n",
      "Result: 90.0\n",
      "Result: 61.0\n",
      "Result: 56.0\n",
      "Result: 54.0\n",
      "Result: 149.0\n",
      "Result: 87.0\n",
      "Result: 57.0\n",
      "Result: 56.0\n",
      "Result: 168.0\n",
      "Result: 50.0\n",
      "Result: 115.0\n",
      "Result: 55.0\n",
      "Result: 74.0\n",
      "Result: 57.0\n",
      "Result: 69.0\n",
      "Result: 47.0\n",
      "Result: 58.0\n",
      "Result: 51.0\n",
      "Result: 70.0\n",
      "Result: 83.0\n",
      "Result: 54.0\n",
      "Result: 58.0\n",
      "Result: 93.0\n",
      "Result: 51.0\n",
      "Result: 148.0\n",
      "Result: 47.0\n",
      "Result: 93.0\n",
      "Result: 54.0\n",
      "Result: 64.0\n",
      "Result: 66.0\n",
      "Result: 54.0\n",
      "Result: 56.0\n",
      "Result: 56.0\n",
      "Result: 51.0\n",
      "Result: 54.0\n",
      "Result: 56.0\n",
      "Result: 62.0\n",
      "Result: 48.0\n",
      "Result: 51.0\n",
      "Result: 53.0\n",
      "Result: 83.0\n",
      "Result: 110.0\n",
      "Result: 83.0\n",
      "Result: 102.0\n",
      "Result: 51.0\n",
      "Result: 52.0\n",
      "Result: 84.0\n",
      "Result: 57.0\n",
      "Result: 95.0\n",
      "Result: 50.0\n",
      "Result: 98.0\n",
      "Result: 52.0\n",
      "Result: 62.0\n",
      "Result: 62.0\n",
      "Result: 71.0\n",
      "Result: 57.0\n",
      "Result: 96.0\n",
      "Result: 53.0\n",
      "Result: 145.0\n",
      "Result: 47.0\n",
      "Result: 58.0\n",
      "Result: 59.0\n",
      "Result: 152.0\n",
      "Result: 55.0\n",
      "Result: 84.0\n",
      "Result: 57.0\n",
      "Result: 53.0\n",
      "Result: 72.0\n",
      "Result: 52.0\n",
      "Result: 50.0\n",
      "Result: 52.0\n",
      "Result: 70.0\n",
      "Result: 53.0\n",
      "Result: 50.0\n",
      "Result: 49.0\n",
      "Result: 64.0\n",
      "Result: 54.0\n",
      "Result: 109.0\n",
      "Result: 82.0\n",
      "Result: 85.0\n",
      "Result: 53.0\n",
      "Result: 93.0\n",
      "Result: 94.0\n",
      "Result: 59.0\n",
      "Result: 62.0\n",
      "Result: 64.0\n",
      "Result: 113.0\n",
      "Result: 53.0\n",
      "Result: 67.0\n",
      "Result: 129.0\n",
      "Result: 63.0\n",
      "Result: 56.0\n",
      "Result: 112.0\n",
      "Result: 114.0\n",
      "Result: 75.0\n",
      "Result: 80.0\n",
      "Result: 55.0\n",
      "Result: 106.0\n",
      "Result: 48.0\n"
     ]
    }
   ],
   "source": [
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"take_cover.cfg\")\n",
    "    game.set_doom_scenario_path(\"take_cover.wad\")\n",
    "    game.init()\n",
    "    left = [1, 0]\n",
    "    right = [0, 1]\n",
    "    actions = [left, right]\n",
    "\n",
    "    episodes = 100\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            frame = preprocess_frame(img)\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            reward = game.make_action(action)\n",
    "#         plot_conv_layer(inputs, frame)\n",
    "#         plot_image(frame)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "#         time.sleep(2)\n",
    "    game.close()\n",
    "game, possible_actions = create_environment()\n",
    "test_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
